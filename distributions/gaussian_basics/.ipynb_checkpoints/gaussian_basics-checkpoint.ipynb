{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE NORMAL DISTRIBUTION\n",
    "\n",
    "*Notes by Andres F.R.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* PDF and CDF for uni and multivariate\n",
    "* Expected value and variance\n",
    "* GMM\n",
    "* KL divergence between 2 gaussians (from simple case to GMM multivar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#import pyro\n",
    "#import pgmpy\n",
    "import matplotlib\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. BASICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Warning: Abuse of notation**: To avoid verbosity, sometimes the symbol $x$ in $P(x)$ can represent a specific value, or any set of events, or the whole set of values that $X$ can take on. This should be made distinguishable enough, usually with an apostrophe meaning a specific value ($x'$).\n",
    "\n",
    "\n",
    "* **Probability distributions** are identified by their random variables, and evaluated on events extracted from the respective variables. i.e. $P_X(7)$ or $P(x')$ is a probability that event $x'$ will occur when the random variable $X$ is \"triggered\" or \"evaluated\". \n",
    "\n",
    "\n",
    "* **Arithmetic**: Given an event, zero probability means impossible, and one means certain: It holds that $0<=P(x)<=1\\; \\forall x \\in X$. And **events behave like sets**. Therefore, \n",
    "  * $P(x_1' \\cup x_2') = P(x_1')+P(x_2')$ for any two disjoint events $x_1'$ and $x_2'$: When rolling a die, the probability that the result is either 2 or 5 equals the sum of both separate probabilities.\n",
    "  * $\\int_X{P(x)dx}=1$: Extending the last example, the probability that rolling a die has any possible outcome is 100%.\n",
    "  * Also note that (for disjoint events) $P(x_1' \\cap x_2') = P(\\varnothing) = 0$. The intersection of two disjoint sets is an empty set, and therefore an impossible outcome (like rolling two numbers simultaneously with a die).\n",
    "\n",
    "\n",
    "* **Joint probability**: over multiple variables: $P(x, y)$. In principle no dependency between X and Y is assumed, therefore it can be regarded as a distribution over a set of linearly independent dimensions (as many as scalar variables). As a consequence, the order of the variables doesn't alter the result (it is **commutative**): $P(x=x', y=y')=P(y=y', x=x') \\; \\iff \\; P(x,y)=P(y,x)$.\n",
    "\n",
    "\n",
    "* **Marginalization**: \"projecting down\" a joint distribution: the impact of a given variable can be neutralized by regarding all its possible events: as seen before, this takes the form of an addition: $P(x, z) = \\int_Y{P(x,y,z) dy}$ in this case Y is marginalized. This process can be continued until no variables are left, and the result of $1$ is achieved (since all possible events in the joint distribution have been regarded).\n",
    "\n",
    "\n",
    "* **Conditional probability**: The conditional relation $X|Y$ (read \"X given Y\") expresses that events in Y have a causal impact in the events in X. $P_{X|Y}(x'|y'))$ (also $P(x=x'|y=y')$) meaning the probability that event $x$ of $X$ is 7 given that event $y$ of $Y$ was 1. Note that $P(a,b,c|d,e)$ expresses the joint probability of \"a,b,c\" given \"d,e\" and **not** the joint probability of a,b,e and \"c given d\".\n",
    "\n",
    "\n",
    "* **Likelihood**: Given $X|Y$, i.e. the outcome of $X$ is conditioned by the prior $Y$ outcome, the likelihood $P(y|x)$ is the probability that the observed event x has been caused by a previous event y. In other words, how likely is that y preceded x. Note that, in contrast with joint relations, $P(y|x)$ is most of the times NOT equal to $P(x|y)$, i.e. conditional relations are **NOT commutative**. Also note that likelihood is a probability distribution itself, and can be treated as such to all effects. We can say that $P(y|x)$\n",
    "\n",
    "\n",
    "* **Joint vs. conditional**: The conditional can be extracted from the joint: $P(x|y=y') = \\frac{P(x, y=y')}{\\int_X{P(x, y=y')dx}}$. Basically $P(x, y=y')$ gives already a proportional answer, but it has to be normalized to add up to 1, hence the division. Also note that the divisor is the **marginalization** of x, therefore the simpler expression holds: $P(x|y=y') = \\frac{P(x, y=y')}{P(y=y')}$. More usually, the compact form is used: $P(x|y)= \\frac{P(x, y)}{P(y)}$.\n",
    "  * Multiple conditioning variables work similarly: $P(X|Y,Z) = \\frac{P(X,Y,Z)}{P(Y,Z)}$\n",
    "\n",
    "\n",
    "* **Conditional chain**: The idea in joint vs. conditional can be extended to more variables:\n",
    "  * $P(a,b) = P(a|b) P(b)$\n",
    "  * $P(a,b,c,d) = P(a,b,c|d)P(d) = P(a,b|c,d)P(c|d)P(d) = P(a |b,c,d)P(b|c,d)P(c|d)P(d)$\n",
    "\n",
    "\n",
    "* **Bayes' rule**: At this point we can finally establish the relation between $P(x|y)$ and $P(y|x)$. Given that:\n",
    "  * $P(x,y) = P(x|y)P(y)$\n",
    "  * $P(y,x) = P(y|x)P(x)$\n",
    "  * $P(x,y)=P(y,x) \\Rightarrow P(x|y)P(y) = P(y|x)P(x)$\n",
    "  * $P(x) = \\int_Y{P(x,y) dy} = \\int_Y\\{ P(x|y)P(y)\\}dy$\n",
    "  * Hence we are able to **bypass the joint distribution and express the likelihood solely in terms of the prior and the marginals**: $P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$, and conversely $P(x|y) = \\frac{P(y|x)P(x)}{P(y)}$.\n",
    "  * The number of required elements can be further reduced as follows:  $P(y|x) = \\frac{P(x|y) P(y)}{\\int_Y\\{ P(x|y)P(y)\\}dy}$ and $P(x|y) = \\frac{P(y|x) P(x)}{\\int_X\\{ P(y|x)P(x)\\}dx}$\n",
    "  * **Example 1**: imagine a setup where we can observe $P(x|y)$, i.e. we see events for X which we assume to be conditioned by Y in some way. We also have an idea of how Y is distributed (so we can compute $P(y)$). Then we can, for a given set of observed events, estimate $P(y|x)$, i.e. the likelihood that a given y' event caused the observed events. For example:\n",
    "    1. I have one fair coin in one pocket (50% heads / 50% tails), and a rigged (80h/20t) coin in the other\n",
    "    2. I pick one coin from a random pocket (50% left / 50% right), I flip it and the outcome is heads.\n",
    "    3. We define $X$ as the coin's distribution, being causally conditioned by me, $Y$, picking a coin at random. We have then $X|Y$.\n",
    "    4. The likelihood of having picked the rigged coin, prior to flipping, is 50%. But after knowing the outcome, the likelihood increases: $P(y=rigged|x=head) = \\frac{P(x=head|y=rigged) P(y=rigged)}{\\sum_Y\\{ P(x=head|y)P(y)\\}} \\Rightarrow P(y=rigged|x=head) = \\frac{0.8 \\cdot 0.5}{0.5\\cdot 0.8 + 0.5 \\cdot 0.5} \\approx 61.5\\%$\n",
    "  * **Example 2**: Note that in the first example we **know** $X|Y$ and $Y$ and therefore we obtain the **actual** likelihood, i.e. guessing the rigged coin 61.5% of the times after observing heads (and 38.5% otherwise) would outperform in the long run a person that guesses always 50/50, even if we know that the rigged coin is pulled exactly 50% of the time. In most problems, we won't know how exactly are $Y$ and $X|Y$ distributed (i.e. *how* the coin extraction works and *how* each coin behaves), and we would have to **estimate** them. E.g., consider you are reading the quarterly report of a company, where there is a chance that it has been manipulated to make profits look bigger. The prior probability that the CEO is lying is $P(y)$, and the distribution of profits and debts $P(x|y)$ contained in the report clearly depends whether the CEO is lying or not. We observe the report only, and want to take it with a grain of salt. Ideally, we would want a quantitative estimation of the likelihood that they are being honest, given the report they provided.\n",
    "    1. First, we need to *define* $P(truth)$, in a way that reflects as good as possible the **prior knowledge that we have about the CEO's honesty**. Let's say that, prior to any observation, we conclude that they are honest 99% of the time.\n",
    "    2. Based on the company's historial and abundant information about the company's business, we also *define* our expectations for $P_{est}(report | honest)$, i.e. the probability that a *honest* report contains certain figures.\n",
    "    3. In the quarterly, we see that the company reported a trillion dollars profit. The fact that this is very unexpected is reflected by a very low value in our model: $P_{est}(trillion|honest)=0.0001\\%$. On the other hand, our model would still consider unlikely, but less, a dishonest trillionaire report: $P_{est}(trillion|dishonest)=2\\%$.\n",
    "    4. Based on our assumptions and observations, we can then compute: $P_{est}(honest|trillion) = \\frac{P_{est}(trillion|honest) \\cdot P_{est}(honest)}{\\sum_Y P_{est}(trillion|y)} = \\frac{0.000001 \\cdot 0.99}{0.99\\cdot 0.000001 + 0.01 \\cdot 0.02} \\approx 0,49\\%$. This reflects a degree of belief that they are being honest with the report, based on our **estimations** of what a honest report looks like and how honest they are in general.\n",
    "  * **Terminology**: After these examples, it should be easier to understand the common terminology that is applied to the four elements in Bayes' rule: $posterior = \\frac{likelihood \\cdot prior}{marginal}$.\n",
    "    1. The **prior** reflects our knowledge of the conditioning variable **before** any observation.\n",
    "    2. The **likelihood** reflects the expectation that a given observation is due to a specific prior event.\n",
    "    3. The **marginal** (also called **evidence**) is a scaling factor that normalizes the result into a probability distribution that sums up to 1.\n",
    "    4. The **posterior** reflects our knowledge of the conditioning variable, **after** observing $x|y$.\n",
    "  \n",
    "  \n",
    "* **Computational cost**: In the second example we see that many estimations are involved, which can affect the quality of the output. Ideally, we want $P_{est}$ to be as close to the real $P$ as possible. In many cases this is a very difficult problem, and in not less cases it requires a great amount of computation. Coming up with good estimators is one of the central problems in this context.\n",
    "\n",
    "\n",
    "* **Independence**: If $X$ and $Y$ are independent, this means that the outcome of event $y'$ won't affect the outcome of a subsequent event $x'$ **and viceversa**. This has to hold **for every possible y'**. This translates into math as follows: $P(x|y)=P(x), \\, P(y|x)=P(y) \\; \\Rightarrow P(x,y)=P(x)P(y)$. In the context of Bayes, independence happens when $posterior=prior$, i.e. $\\frac{likelihood}{marginal} = 1$. In the coin example, this would require all coins to be identically distributed, e.g. equally rigged, which forces $marginal=likelihood\\cdot 1$ as it can be seen: $P(rigged|head) = P(rigged)\\frac{P(head|rigged)}{P(head|rigged)\\cdot \\sum_Y P(y)} = P(rigged)\\frac{P(head|rigged)}{P(head|rigged) \\cdot 1} = P(rigged)$ Note that $P(head|rigged)$ can be taken out of the sum as a constant, since (per definition) it isn't affected by $P(y)$, and the marginalization of $P(y)$ is (also per definition) one. Also note that this has to hold for every possible state: in the case of the coins it is heads and tails, in the case of dices numbers 1 to 6, etc.\n",
    "\n",
    "\n",
    "* **Expected value**: Expressed with $\\mathbb{E}[X]$,it is a property of a *random variable*. It doesn't make sense to compute $E$ for a set of events. It provides the average outcome to be obtained if the variable was evaluated \"infinite\" times.\n",
    "  * In its simplest form, $E[X] = \\int_X\\{ x P(x)\\}dx$. E.g., $E[die] = \\frac{1}{6}+\\frac{2}{6}+\\frac{3}{6}+\\frac{4}{6}+\\frac{5}{6}+\\frac{6}{6} = 3.5$\n",
    "  * The simpler version of the conditional $E[X|Y]$ is a value if $Y$ takes indeed the form of a single value $y'$: $E[X|Y=y'] = \\int_X\\{x \\cdot P(x|y')\\}dx$.\n",
    "  * If we don't constraint the outcome of $Y$, the expected value will be a function of that outcome and therefore a random variable itself. Particularly, the function of $Y$ that best approximates $X$. See the Arizona Uni document for more details.\n",
    "  * Let $X,Y,Z$ be RVs,  $a,b \\in \\mathbb{R}$, and $g: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "  * $E[a|Y] = a$\n",
    "  * $E[aX+bZ|Y] = aE[X|Y] + bE[Z|Y]$ conditional E is multilinear too.\n",
    "  * $X \\geq 0 \\Rightarrow E[X|Y]\\geq 0$\n",
    "  * $E[X|Y] = E[X]$ if $X \\perp Y$\n",
    "  * $E[E[X|Y]] = E[X]$ this is a marginalization disguised.\n",
    "  * $E[Xg(Y) | Y] = g(Y)E[X|Y]$. Also, $E[g(Y)|Y]=g(Y)$. This is so because g(Y) affects values but not probabilities and can get factored out of the E sum.\n",
    "  * $E[X| Y, g(Y)] = E[X,Y]$ because only *which* $y'$ and *its probability* matter to $X$.\n",
    "  * $E[E[X|Y;Z] | Y=y] = E[X|Y=y]$ This one works by expanding both expressions and reformulating the conditionals in terms of joints and marginals: the terms containing $Z$ get cancelled out.\n",
    "  * $E[X|Y]$ is the function of $Y$ that best approximates $X$. I.e., $E[(X-E[X|Y])^2] <= E[(X-g(Y))^2]$ \n",
    "\n",
    "\n",
    "* **Variance**: simple and conditional\n",
    "\n",
    "\n",
    "* **Covariance**:\n",
    "\n",
    "\n",
    "* **Nested bayes**: x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF\n",
    "\n",
    "### Univariate:\n",
    "\n",
    " $P(X|Y,Z) = \\frac{P(X,Y,Z)}{P(Y,Z)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = 1\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha=\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = 1\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha=\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = 1\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha=\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = 1\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha=\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = 1\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha=\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = 1\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
